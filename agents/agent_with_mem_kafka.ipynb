{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d07c25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /workspaces/fraud_detection_demo/agents\n",
      "Project root on sys.path: /workspaces/fraud_detection_demo\n",
      "Using .env: /workspaces/fraud_detection_demo/.env\n",
      "MONGODB_URI seen by Settings: mongodb+srv://anant_test:YNmLExA6nnFM3f0y@cluster0.kzo3h.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\n"
     ]
    }
   ],
   "source": [
    "# --- Notebook bootstrap: make imports + .env work from anywhere in the repo ---\n",
    "\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Put the Git repo root on sys.path (so `config.settings` resolves from anywhere)\n",
    "def find_git_root(start: Path) -> Path:\n",
    "    p = start\n",
    "    while p != p.parent:\n",
    "        if (p / \".git\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    return start\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = find_git_root(CWD)\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"CWD:\", CWD)\n",
    "print(\"Project root on sys.path:\", ROOT)\n",
    "\n",
    "# 2) Load the .env from the repo root (override existing env in the kernel)\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    # Prefer .env at the project root; fall back to a discovered one\n",
    "    env_path = (ROOT / \".env\")\n",
    "    if not env_path.exists():\n",
    "        # find_dotenv(usecwd=True) searches upward from CWD\n",
    "        found = find_dotenv(usecwd=True)\n",
    "        env_path = Path(found) if found else None\n",
    "\n",
    "    print(\"Using .env:\", env_path if env_path else \"(none found)\")\n",
    "    if env_path:\n",
    "        load_dotenv(dotenv_path=env_path, override=True)\n",
    "except Exception as e:\n",
    "    print(\"dotenv not available or failed:\", e)\n",
    "\n",
    "# 3) Reload settings so it re-reads environment (critical if it was imported earlier)\n",
    "import config.settings as cfg\n",
    "importlib.reload(cfg)\n",
    "\n",
    "# 4) Access settings (supports either pattern you may have):\n",
    "Settings = getattr(cfg, \"get_settings\", None)\n",
    "if callable(Settings):\n",
    "    Settings = Settings()  # pydantic factory pattern\n",
    "else:\n",
    "    Settings = cfg.Settings  # module-level instance/class\n",
    "\n",
    "print(\"MONGODB_URI seen by Settings:\", getattr(Settings, \"MONGODB_URI\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "611655cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching modules under: /workspaces/fraud_detection_demo\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root: one level up from /agents\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent.parent if \"__file__\" in globals() else Path.cwd().parent\n",
    "\n",
    "# Ensure it's on sys.path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Searching modules under:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d17ad776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymongo import MongoClient\n",
    "from models.transaction import Transaction\n",
    "from services.fraud_signature_service import FraudSignatureService\n",
    "from services.cloud_kafka_service import CloudKafkaService\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "import logging\n",
    "import asyncio\n",
    "from config.settings import Settings\n",
    "from config.logger import get_logger\n",
    "from datetime import datetime\n",
    "from utils.print_helper import *\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06deb5c",
   "metadata": {},
   "source": [
    "### \n",
    "1. MongoDB Connection\n",
    "2. Encoding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dba2b7cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(__name__)\n",
    "client = MongoClient(Settings.MONGODB_URI)\n",
    "encoder = SentenceTransformer(Settings.EMBEDDING_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aa3de2ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ensure_state(s: Union[\"WorkflowState\", dict]) -> \"WorkflowState\":\n",
    "    return s if isinstance(s, WorkflowState) else WorkflowState(**s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c3f93",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "Pydantic model\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "129872c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class WorkflowState(BaseModel):\n",
    "    transaction: Transaction\n",
    "    duplicate_check: Optional[Dict[str, Any]] = None\n",
    "    fraud_analysis: Optional[Dict[str, Any]] = None\n",
    "    similarity_data: Optional[Dict[str, Any]] = None\n",
    "    recommendation: Optional[str] = None\n",
    "    storage_status: Optional[str] = None\n",
    "    errors: List[str] = Field(default_factory=list)  # avoid shared mutable default\n",
    "    agent_reflection: Optional[str] = None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def convert_transaction_if_needed(cls, values):\n",
    "        txn = values.get(\"transaction\")\n",
    "        if isinstance(txn, dict):\n",
    "            values[\"transaction\"] = Transaction(**txn)\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ad980a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    return client[Settings.MONGODB_DATABASE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f92b",
   "metadata": {},
   "source": [
    "### Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5b0f7c67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def duplicate_check(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        transaction = state.transaction\n",
    "\n",
    "        def check_duplicate():\n",
    "            db = get_db()\n",
    "            return db[Settings.MONGODB_COLLECTION_CASES].count_documents(\n",
    "                {\"ip_address\": transaction.ip_address}\n",
    "            ) > 0\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        is_duplicate = await loop.run_in_executor(None, check_duplicate)\n",
    "\n",
    "        result = {\n",
    "            \"is_duplicate\": is_duplicate,\n",
    "            \"customer_id\": transaction.customer_id,\n",
    "            \"transaction_id\": transaction.transaction_id,\n",
    "            \"checked_at\": time.time(),  # epoch seconds (fix)\n",
    "            \"recommendation\": \"SKIP_PROCESSING\" if is_duplicate else \"CONTINUE_PROCESSING\"\n",
    "        }\n",
    "\n",
    "        if is_duplicate:\n",
    "            result[\"reason\"] = \"Duplicate complaint detected within 24 hours\"\n",
    "\n",
    "        state.duplicate_check = result\n",
    "        logger.info(f\"Duplicate check completed for {transaction.transaction_id}: {is_duplicate}\")\n",
    "\n",
    "        # Pretty print\n",
    "        pretty_duplicate(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        state.errors.append(f\"duplicate_check error: {str(e)}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c7a3",
   "metadata": {},
   "source": [
    "### Fraud Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80c241ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def fraud_classification(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        amt = state.transaction.amount or 0\n",
    "        state.fraud_analysis = {\"is_fraud\": amt > 1000}\n",
    "        logger.info(f\"Fraud classification: {state.fraud_analysis}\")\n",
    "\n",
    "        # Pretty print\n",
    "        pretty_fraud(state.fraud_analysis, amt)\n",
    "\n",
    "    except Exception as e:\n",
    "        state.errors.append(f\"fraud_classification error: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0134d6c",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6df414c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def similarity_search(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        fs = FraudSignatureService()\n",
    "        signatures = fs.generate_fraud_signatures(state.transaction)\n",
    "\n",
    "        def encode():\n",
    "            return encoder.encode([\" \".join(signatures)])[0]\n",
    "\n",
    "        embedding = await loop.run_in_executor(None, encode)\n",
    "\n",
    "        def search():\n",
    "            db = get_db()\n",
    "            results = list(db[Settings.MONGODB_COLLECTION_SIGNATURES].aggregate([\n",
    "                {\"$vectorSearch\": {\n",
    "                    \"index\": Settings.VECTOR_INDEX_NAME,\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"queryVector\": embedding.tolist(),\n",
    "                    \"numCandidates\": 50,\n",
    "                    \"limit\": 5,\n",
    "                }},\n",
    "                # If your server supports it, you can $project score explicitly:\n",
    "                # {\"$project\": {\"_id\": 1, \"transaction_id\": 1, \"signatures\": 1, \"score\": {\"$meta\": \"vectorSearchScore\"}}}\n",
    "            ]))\n",
    "\n",
    "            # Convert ObjectId to string for clean printing\n",
    "            for r in results:\n",
    "                if \"_id\" in r:\n",
    "                    r[\"_id\"] = str(r[\"_id\"])\n",
    "            return results\n",
    "\n",
    "        results = await loop.run_in_executor(None, search)\n",
    "\n",
    "        # Save both the results and the query signatures for printing\n",
    "        state.similarity_data = {\"similar_cases\": results, \"query_signatures\": signatures}\n",
    "\n",
    "        # Store signature for high-risk transactions\n",
    "        def insert_signature():\n",
    "            db = get_db()\n",
    "            db[Settings.MONGODB_COLLECTION_SIGNATURES].insert_one({\n",
    "                \"transaction_id\": state.transaction.transaction_id,\n",
    "                \"signatures\": signatures,\n",
    "                \"embedding\": embedding.tolist()\n",
    "            })\n",
    "\n",
    "        if state.fraud_analysis and state.fraud_analysis.get(\"is_fraud\"):\n",
    "            await loop.run_in_executor(None, insert_signature)\n",
    "            logger.info(f\"Stored fraud signature for {state.transaction.transaction_id}\")\n",
    "\n",
    "        # Pretty print\n",
    "        pretty_similarity(state.similarity_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        state.errors.append(f\"similarity_search error: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043d3a1",
   "metadata": {},
   "source": [
    "### LLM Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "661deb1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def agent_reflection(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        # Use GPT-3.5-turbo for better rate limits and lower cost\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            temperature=0,\n",
    "            max_retries=3,\n",
    "            request_timeout=60,\n",
    "            max_tokens=200  # Significantly reduce response length\n",
    "        )\n",
    "        \n",
    "        # Create a much shorter, focused prompt\n",
    "        fraud_status = state.fraud_analysis.get(\"is_fraud\", False) if state.fraud_analysis else False\n",
    "        similar_count = len(state.similarity_data.get(\"similar_cases\", [])) if state.similarity_data else 0\n",
    "        \n",
    "        prompt = f\"\"\"Fraud detected: {fraud_status}\n",
    "Similar cases found: {similar_count}\n",
    "\n",
    "Brief analysis (max 100 words): What actions should be taken?\"\"\"\n",
    "        \n",
    "        # Add delay to prevent rate limiting\n",
    "        await asyncio.sleep(2)\n",
    "        \n",
    "        response = await llm.ainvoke(prompt)\n",
    "        state.agent_reflection = str(response.content if hasattr(response, \"content\") else response)\n",
    "        logger.info(\"Agent reflection completed\")\n",
    "        \n",
    "        # Pretty print\n",
    "        pretty_reflection(state.agent_reflection)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"agent_reflection error (continuing): {str(e)}\")\n",
    "        # Provide a simple fallback response\n",
    "        fraud_status = state.fraud_analysis.get(\"is_fraud\", False) if state.fraud_analysis else False\n",
    "        state.agent_reflection = (\n",
    "            \"Automatic analysis: High risk transaction - requires immediate review\"\n",
    "            if fraud_status else\n",
    "            \"Automatic analysis: Low risk transaction - standard processing\"\n",
    "        )\n",
    "        state.errors.append(f\"agent_reflection error: {str(e)}\")\n",
    "        pretty_reflection(state.agent_reflection)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644ab0f",
   "metadata": {},
   "source": [
    "### LLM Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a92602c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def action_recommendation(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        # Use GPT-3.5-turbo for better rate limits and lower cost\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            temperature=0,\n",
    "            max_retries=3,\n",
    "            request_timeout=60,\n",
    "            max_tokens=150  # Significantly reduce response length\n",
    "        )\n",
    "        \n",
    "        # Create a much shorter, focused prompt\n",
    "        fraud_status = state.fraud_analysis.get(\"is_fraud\", False) if state.fraud_analysis else False\n",
    "        similar_count = len(state.similarity_data.get(\"similar_cases\", [])) if state.similarity_data else 0\n",
    "        \n",
    "        prompt = f\"\"\"Transaction risk: {'HIGH' if fraud_status else 'LOW'}\n",
    "Similar cases: {similar_count}\n",
    "\n",
    "Recommended action (max 50 words):\"\"\"\n",
    "        \n",
    "        # Add delay to prevent rate limiting\n",
    "        await asyncio.sleep(2)\n",
    "        \n",
    "        response = await llm.ainvoke(prompt)\n",
    "        state.recommendation = str(response.content if hasattr(response, \"content\") else response)\n",
    "        logger.info(\"Action recommendation completed\")\n",
    "        \n",
    "        # Pretty print\n",
    "        pretty_recommendation(state.recommendation)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"action_recommendation error (continuing): {str(e)}\")\n",
    "        # Provide a simple fallback response\n",
    "        fraud_status = state.fraud_analysis.get(\"is_fraud\", False) if state.fraud_analysis else False\n",
    "        if fraud_status:\n",
    "            state.recommendation = \"RECOMMENDED ACTION: Block transaction, investigate customer account, review for fraud patterns.\"\n",
    "        else:\n",
    "            state.recommendation = \"RECOMMENDED ACTION: Approve transaction, continue monitoring.\"\n",
    "        state.errors.append(f\"action_recommendation error: {str(e)}\")\n",
    "        pretty_recommendation(state.recommendation)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca24cc",
   "metadata": {},
   "source": [
    "### Store Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b905b6fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def store_transaction(state: WorkflowState) -> WorkflowState:\n",
    "    try:\n",
    "        def store():\n",
    "            db = get_db()\n",
    "            # Handle both Pydantic v1 and v2\n",
    "            if hasattr(state.transaction, 'model_dump'):\n",
    "                txn_dict = state.transaction.model_dump()\n",
    "            else:\n",
    "                txn_dict = state.transaction.dict()\n",
    "            db[Settings.MONGODB_COLLECTION_CASES].insert_one(txn_dict)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        await loop.run_in_executor(None, store)\n",
    "        state.storage_status = \"stored\"\n",
    "        logger.info(f\"Transaction {state.transaction.transaction_id} stored successfully\")\n",
    "        \n",
    "        # Pretty print\n",
    "        pretty_storage(state.storage_status, state.transaction.transaction_id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        state.errors.append(f\"storage error: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b9156",
   "metadata": {},
   "source": [
    "### Kafka Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c1fbc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def publish_result(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Publish the complete workflow result to Kafka output topic\"\"\"\n",
    "    try:\n",
    "        # Helper function to serialize datetime objects\n",
    "        def serialize_datetime(obj):\n",
    "            if isinstance(obj, datetime):\n",
    "                return obj.isoformat() + \"Z\"\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: serialize_datetime(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [serialize_datetime(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        # Create a comprehensive result payload\n",
    "        result_payload = {\n",
    "            \"transaction_id\": state.transaction.transaction_id,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "            \"transaction\": serialize_datetime(state.transaction.model_dump() if hasattr(state.transaction, 'model_dump') else state.transaction.dict()),\n",
    "            \"fraud_analysis\": serialize_datetime(state.fraud_analysis),\n",
    "            \"similarity_data\": serialize_datetime(state.similarity_data),\n",
    "            \"recommendation\": state.recommendation,\n",
    "            \"agent_reflection\": state.agent_reflection,\n",
    "            \"storage_status\": state.storage_status,\n",
    "            \"errors\": state.errors\n",
    "        }\n",
    "        \n",
    "        # Use the existing Kafka service to publish\n",
    "        kafka_service = CloudKafkaService()\n",
    "        success = kafka_service.publish_message(\n",
    "            key=state.transaction.transaction_id,\n",
    "            data=result_payload,\n",
    "            topic=Settings.KAFKA_OUTPUT_TOPIC\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            logger.info(f\"Published result for transaction {state.transaction.transaction_id}\")\n",
    "        else:\n",
    "            state.errors.append(\"Failed to publish result to Kafka\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        state.errors.append(f\"publish_result error: {str(e)}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed794fa9",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a4621df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_workflow(checkpointer):\n",
    "    \"\"\"Build the LangGraph workflow with MongoDB checkpointing\"\"\"\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"duplicate\", duplicate_check)\n",
    "    workflow.add_node(\"classify\", fraud_classification)\n",
    "    workflow.add_node(\"similarity\", similarity_search)\n",
    "    workflow.add_node(\"reflect\", agent_reflection)\n",
    "    workflow.add_node(\"recommend\", action_recommendation)\n",
    "    workflow.add_node(\"store\", store_transaction)\n",
    "    workflow.add_node(\"publish\", publish_result)\n",
    "\n",
    "    # Define the workflow\n",
    "    workflow.set_entry_point(\"duplicate\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"duplicate\",\n",
    "        lambda s: \"skip\" if s.duplicate_check and s.duplicate_check.get(\"is_duplicate\") else \"classify\",\n",
    "        {\"skip\": \"publish\", \"classify\": \"classify\"}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"classify\",\n",
    "        lambda s: \"similarity\" if s.fraud_analysis and s.fraud_analysis.get(\"is_fraud\") else \"publish\",\n",
    "        {\"similarity\": \"similarity\", \"publish\": \"publish\"}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"similarity\", \"reflect\")\n",
    "    workflow.add_edge(\"reflect\", \"recommend\")\n",
    "    workflow.add_edge(\"recommend\", \"store\")\n",
    "    workflow.add_edge(\"store\", \"publish\")\n",
    "    workflow.add_edge(\"publish\", END)\n",
    "\n",
    "    # Compile with MongoDB checkpointer\n",
    "    return workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409866b",
   "metadata": {},
   "source": [
    "### Kick of the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c19ccd1f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def run_workflow_with_checkpoint(compiled_workflow, transaction_data: dict, thread_id: str):\n",
    "    \"\"\"Run workflow with proper checkpoint handling\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if there's an existing checkpoint\n",
    "        existing_state = await compiled_workflow.aget_state(config)\n",
    "        \n",
    "        if existing_state and existing_state.values:\n",
    "            logger.info(f\"Resuming workflow from checkpoint for thread: {thread_id}\")\n",
    "            final_state = await compiled_workflow.ainvoke(None, config=config)\n",
    "        else:\n",
    "            logger.info(f\"Starting new workflow for thread: {thread_id}\")\n",
    "            # Start fresh workflow\n",
    "            start_state = ensure_state({\"transaction\": transaction_data})\n",
    "            # Kick it off\n",
    "            final_state = await compiled_workflow.ainvoke(start_state, config=config)\n",
    "        \n",
    "        # After END, ainvoke returns a dict—hydrate for printing/return\n",
    "        hydrated = ensure_state(final_state)\n",
    "        pretty_final(hydrated)\n",
    "        return hydrated\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in workflow execution for {thread_id}: {e}\")\n",
    "        # If checkpoint resume fails, try starting fresh\n",
    "        try:\n",
    "            logger.info(f\"Attempting fresh start for thread: {thread_id}\")\n",
    "            start_state = ensure_state({\"transaction\": transaction_data})\n",
    "            final_state = await compiled_workflow.ainvoke(start_state, config=config)\n",
    "            hydrated = ensure_state(final_state)\n",
    "            pretty_final(hydrated)\n",
    "            return hydrated\n",
    "        except Exception as fresh_error:\n",
    "            logger.error(f\"Fresh start also failed for {thread_id}: {fresh_error}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "64db4842",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def process_transaction_from_kafka(transaction: Transaction, compiled_workflow):\n",
    "    \"\"\"Process a single transaction from Kafka through the workflow with checkpointing\"\"\"\n",
    "    try:\n",
    "        thread_id = f\"txn_{transaction.transaction_id}\"\n",
    "        \n",
    "        # Convert Transaction object to dict for workflow processing\n",
    "        if hasattr(transaction, 'model_dump'):\n",
    "            transaction_dict = transaction.model_dump()\n",
    "        else:\n",
    "            transaction_dict = transaction.dict()\n",
    "        \n",
    "        final_state = await run_workflow_with_checkpoint(\n",
    "            compiled_workflow, \n",
    "            transaction_dict, \n",
    "            thread_id\n",
    "        )\n",
    "        \n",
    "        return final_state\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing transaction {transaction.transaction_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "31b3e9ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def kafka_transaction_callback(transaction: Transaction, compiled_workflow):\n",
    "    \"\"\"Async callback function for Kafka consumer to process transactions with checkpointing\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nProcessing transaction from Kafka: {transaction.transaction_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        result = await process_transaction_from_kafka(transaction, compiled_workflow)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"Workflow completed successfully for transaction: {transaction.transaction_id}\")\n",
    "        else:\n",
    "            print(f\"Workflow failed for transaction: {transaction.transaction_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Kafka callback: {e}\")\n",
    "        print(f\"Kafka callback error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593940c",
   "metadata": {},
   "source": [
    "### Start Kafka consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e28befc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def consume_transactions_async(kafka_service, compiled_workflow):\n",
    "    \"\"\"Async version of consume_transactions that processes messages asynchronously\"\"\"\n",
    "    try:\n",
    "        if not kafka_service.consumer:\n",
    "            kafka_service.create_consumer()\n",
    "        \n",
    "        # Setup schema registry if not already done\n",
    "        if not kafka_service.schema_registry_client:\n",
    "            kafka_service.setup_schema_registry()\n",
    " \n",
    "        poll_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                poll_count += 1\n",
    "                logger.info(f\"Poll #{poll_count} - waiting for message...\")\n",
    "                \n",
    "                msg = kafka_service.consumer.poll(timeout=1.0)\n",
    "                \n",
    "                if msg is None:\n",
    "                    logger.info(f\"   Poll #{poll_count}: No message received\")\n",
    "                    continue\n",
    "                \n",
    "                if msg.error():\n",
    "                    logger.error(f\"   Poll #{poll_count}: Consumer error: {msg.error()}\")\n",
    "                    continue\n",
    "                \n",
    "                # Parse message \n",
    "                message_bytes = msg.value()\n",
    "                logger.info(f\"   Message bytes: {message_bytes[:20]}... (first 20 bytes)\")\n",
    "\n",
    "                # Parse message using manual Schema Registry format detection\n",
    "                # Check if it's a Schema Registry format message\n",
    "                if len(message_bytes) >= 5 and message_bytes[0:2] == b'\\x00\\x00':\n",
    "                    # Schema Registry format: [0, 0, schema_id_high, schema_id_low, ...json_data]\n",
    "                    try:\n",
    "                        # Extract JSON data (skip the 5-byte header)\n",
    "                        json_data = message_bytes[5:].decode('utf-8')\n",
    "                        transaction_dict = json.loads(json_data)\n",
    "                    except Exception as manual_error:\n",
    "                        logger.error(f\"Manual Schema Registry parsing failed: {manual_error}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Try plain JSON deserialization\n",
    "                    try:\n",
    "                        transaction_dict = json.loads(msg.value().decode('utf-8'))\n",
    "                    except UnicodeDecodeError as decode_error:\n",
    "                        logger.error(f\"Failed to decode message as UTF-8: {decode_error}\")\n",
    "                        logger.error(\"   Message appears to be binary but not Schema Registry format\")\n",
    "                        continue\n",
    "                    except json.JSONDecodeError as json_error:\n",
    "                        logger.error(f\"Failed to parse JSON: {json_error}\")\n",
    "                        continue\n",
    "                \n",
    "                try:\n",
    "                    transaction = Transaction(**transaction_dict)\n",
    "                    \n",
    "                    # Process transaction asynchronously\n",
    "                    await kafka_transaction_callback(transaction, compiled_workflow)\n",
    "                except Exception as transaction_error:\n",
    "                    logger.error(f\"Failed to create Transaction object: {transaction_error}\")\n",
    "                    logger.error(f\"   Transaction dict: {transaction_dict}\")\n",
    "                    continue\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Received keyboard interrupt, shutting down...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in message processing loop: {e}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in async consumer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b19c8a6e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main execution function using async context manager with Kafka consumption\"\"\"\n",
    "    \n",
    "    # Use async context manager for MongoDB checkpointer\n",
    "    async with AsyncMongoDBSaver.from_conn_string(\n",
    "        conn_string=Settings.MONGODB_URI,\n",
    "        db_name=Settings.MONGODB_DATABASE,\n",
    "        collection_name=\"langgraph_checkpoints\"\n",
    "    ) as checkpointer:\n",
    "        \n",
    "        logger.info(\"MongoDB checkpointer initialized successfully\")\n",
    "        \n",
    "        try:\n",
    "            # Build the workflow with MongoDB checkpointing\n",
    "            compiled_workflow = build_workflow(checkpointer)\n",
    "            logger.info(\"Workflow compiled successfully with MongoDB checkpointer\")\n",
    "            \n",
    "            # Initialize Kafka service\n",
    "            kafka_service = CloudKafkaService()\n",
    "            \n",
    "            print(\"Starting Kafka consumer for fraud detection workflow with memory...\")\n",
    "            print(f\"Listening on topic: {Settings.KAFKA_TOPIC}\")\n",
    "            print(\"Press Ctrl+C to stop...\")\n",
    "            \n",
    "            # Start consuming transactions from Kafka asynchronously\n",
    "            await consume_transactions_async(kafka_service, compiled_workflow)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nShutting down...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in main execution: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Clean up Kafka service\n",
    "            if 'kafka_service' in locals():\n",
    "                kafka_service.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e74ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:25:37 | INFO     | __main__ | MongoDB checkpointer initialized successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MongoDB checkpointer initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 11:25:37 | INFO     | __main__ | Workflow compiled successfully with MongoDB checkpointer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Workflow compiled successfully with MongoDB checkpointer\n",
      "INFO:services.cloud_kafka_service:Consumer object created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kafka consumer for fraud detection workflow with memory...\n",
      "Listening on topic: fraudulent-transactions\n",
      "Press Ctrl+C to stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%3|1755170737.856|FAIL|rdkafka#consumer-1| [thrd:sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.c]: sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.cloud:9092/bootstrap: Failed to resolve 'your-kafka-cluster.region.cloud-provider.confluent.cloud:9092': Name or service not known (after 140ms in state CONNECT)\n",
      "%3|1755170739.041|FAIL|rdkafka#consumer-1| [thrd:sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.c]: sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.cloud:9092/bootstrap: Failed to resolve 'your-kafka-cluster.region.cloud-provider.confluent.cloud:9092': Name or service not known (after 149ms in state CONNECT, 1 identical error(s) suppressed)\n",
      "%3|1755170770.028|FAIL|rdkafka#consumer-1| [thrd:sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.c]: sasl_ssl://your-kafka-cluster.region.cloud-provider.confluent.cloud:9092/bootstrap: Failed to resolve 'your-kafka-cluster.region.cloud-provider.confluent.cloud:9092': Name or service not known (after 11ms in state CONNECT, 30 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Run the main function\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9be3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
